# TangoBotTrainer Project Knowledge Base

## Table of Contents

1. **Introduction**
   1.1 Purpose of the Document
   1.2 Project Overview
   1.3 Goals and Objectives

2. **Conceptual Overview**
   2.1 Agent
   2.2 Neural Network (NN)
   2.3 Genome
   2.4 Neural Network Runtime (NNRT)

3. **Design Guidelines**
   3.1 Agent Design
   3.2 Neural Network Design
   3.3 Genome Design
   3.4 Neural Network Runtime Design

4. **Implementation Details**
   4.1 Agent Implementation
   4.2 Neural Network Implementation
   4.3 Genome Implementation
   4.4 Neural Network Runtime Implementation

5. **Abstract Concepts**
   5.1 Evolutionary Algorithms
   5.2 Neural Network Training
   5.3 Perceptors and Actuators
   5.4 Environment Interaction
   5.5 Evaluators
   5.6 Evolution Process

6. **Development Practices**
   6.1 Coding Standards
   6.2 Testing and Validation
   6.3 Version Control
   6.4 Documentation

7. **TangoBot Training Project (TBTP)**
   7.1 Project Structure
   7.2 Key Components

8. **Application Components**
   8.1 UI
   8.2 Workflow
   8.3 TestingBooth
   8.4 Agent
   8.5 Environment
   8.6 Neural Network Runtime (NNRT)
   8.7 Evaluators
   8.8 DataManager

9. **Appendices**
   9.1 Glossary of Terms
   9.2 References and Resources
   9.3 Example Code Snippets

## 1. Introduction

### 1.1 Purpose of the Document
The TangoBot Trainer Project Document serves as a central knowledge base for all technical teams involved in the design, development, and theoretical foundation of TangoBot. It provides a structured reference to ensure alignment across teams, offering a clear understanding of the project's goals, architecture, implementation, and evolutionary learning mechanisms.

This document is intended for engineers, AI researchers, developers, and system architects working on the TangoBot ecosystem. It excludes corporate concerns such as accounting, marketing, budgeting, business development, and procurement, as these are outside the scope of technical execution.

### 1.2 Project Overview
Provide a brief overview of the TangoBotTrainer project, including its purpose and scope.

### 1.3 Goals and Objectives
Outline the primary goals and objectives of the project.

## 2. Conceptual Overview

### 2.1 Agent

An Agent in the TangoBotTrainer project is a metadata definition created by the user through a GUI. It represents the physical body of a specimen, while the Neural Network (NN) represents the brain. The Agent interacts with the environment through its perceptors and actuators, which serve as its senses and limbs, respectively.

#### Properties of the Agent

1. **string Name**:
   - The name of the agent.
   - This value is assigned at the time of agent creation and remains constant.

2. **string Description**:
   - A description of the agent.
   - This value provides information about the agent's purpose and functionality.

3. **List<IPerceptor> Perceptors**:
   - A collection of perceptors associated with the agent.
   - Perceptors are sensors that receive input from the environment.
   - Each perceptor specifies:
     - **string Name**: The name of the perceptor for reporting purposes.
     - **string Description**: A description of the perceptor.
     - **Type DataType**: The data type that is going to be received (e.g., double, int, array, image, etc.).

4. **List<IActuator> Actuators**:
   - A collection of actuators associated with the agent.
   - Actuators perform actions on the environment based on the data received by the agent from the neural network's output nodes.
   - Each actuator specifies:
     - **string Name**: The name of the actuator for reporting purposes.
     - **string Description**: A description of the actuator.
     - The output is a double value from -1 to 1. The agent runtime component must implement an interpreter that converts the output value into an action that it will exercise on the Environment interface (e.g., a Trading Platform).

5. **IEnvInterface EnvInterface**:
   - The environment interface that the agent interacts with.
   - This interface defines how the agent communicates with the environment.

6. **ITestingBooth TestingBooth**:
   - The testing booth used to evaluate the agent's performance.
   - This component is responsible for running tests and collecting performance metrics.

#### Methods of the Agent

1. **void Initialize()**:
   - Initializes the agent and its components.
   - Sets up the perceptors, actuators, and environment interface.

2. **void UpdatePerceptors(Dictionary<string, object> data)**:
   - Updates the agent's perceptors with new input data from the environment.
   - Processes the input data, normalizes it, and stores it in the perceptors.
   - The normalization of all the values received through the perceptors is the responsibility of the Agent's runtime component.
   - The agent waits passively for the data to come from the TestingBooth. The update frequency of the agent's perceptors is determined by the TestingBooth.
   - The agent preprocesses and normalizes the data and distributes these values to the NN input nodes by passing them to the Neural Network Runtime exposed method `Input(Dictionary<string, double> processedData)`.

#### Runtime Component

- At runtime, a `RuntimeAgentComponent` is instantiated using the metadata as input for the instantiated agent class.
- On each project, only one agent should be defined, although many agents will be instantiated at runtime, each one with its respective Neural Network as a brain.
- The agent represents the physical body of a specimen, while the Neural Network represents the brain.
- The perceptors represent the senses, while the actuators represent the arms, hands, legs, or any other means by which the specimen can interact with the environment.
- Inside the agent lives a Neural Network. The NN is a metadata that describes nodes and connections created after the genome. To execute the NN, it is wrapped in a Neural Network Runtime that processes the received information through the input and updates a property to which the Agent listens.
- The agent constantly listens to the `Output` property of the Neural Network Runtime. This property is an observable object (Dictionary<outputNode, double>).
- When the `Output` property is updated within the Neural Network Runtime, the agent interprets the data contained and turns it into actions.
- For each item in the `Output` object (Dictionary<outputNode, double>), the designer must include a script that will interpret the value and turn it into action performed on the Environment Object, which is passed as context to the script.

#### Perceptor Definition

A Perceptor is a sensor associated with the agent that receives input from the environment. Each Perceptor has the following properties:

1. **string Name**:
   - The name of the Perceptor for reporting purposes.

2. **string Description**:
   - A description of the Perceptor.

3. **Type DataType**:
   - The data type that is going to be received by the Perceptor (e.g., double, int, array, image, etc.).

Perceptors are responsible for capturing environmental data and passing it to the agent for processing. The data received by the Perceptors is normalized and used as input for the Neural Network Runtime.

#### Actuator Definition

An Actuator is a component associated with the agent that performs actions on the environment based on the data received from the neural network's output nodes. Each Actuator has the following properties:

1. **string Name**:
   - The name of the Actuator for reporting purposes.

2. **string Description**:
   - A description of the Actuator.

3. **double Output**:
   - The output value from the neural network, ranging from -1 to 1. This value is interpreted by the agent's runtime component to perform an action on the environment.

4. **string Script**:
   - A script that interprets the output value and turns it into an action performed on the Environment Object. This script can be a non-compiled source.

The agent runtime component must implement an interpreter that converts the output value into a specific action that it will exercise on the Environment interface (e.g., a Trading Platform). For each item in the `Output` object (Dictionary<outputNode, double>), the designer must include a script that will interpret the value and turn it into action performed on the Environment Object, which is passed as context to the script.

### 2.2 Neural Network (NN)
Explain the concept of a Neural Network, its structure, and its purpose.

### 2.3 Genome

A Genome in the TangoBotTrainer project defines the structure and properties of the genetic representation of a neural network. It is responsible for encoding the neural network's architecture and facilitating evolutionary processes such as mutation and crossover.

#### Properties of the Genome

1. **int CurrentModuleId**:
   - This value is set by the Campaign object when a new cycle starts.
   - Genes created during the current cycle will hold this number permanently.
   - This value is an integer because at each cycle a new value (1, 2, 3, ...) is given.
   - The cycle is a part of the evolutive stage where a skill or set of skills is developed by the genome.
   - The value is called a module because it refers to the genes structure developed during a specific cycle.
   - In the neural network, the genes structure developed during, for instance, Cycle 1, will carry the ModuleId 1.
   - This helps during evolution mutations by preventing old modules that are already trained and well developed from being modified by later mutations, thus avoiding the issue of "forgetting."

2. **DateTime TimeCreated**:
   - The time at which the Genome was created. This helps determine the age of the genome.
   - This value is entered at instantiation.

3. **List<IGene> Genes**:
   - The collection of genes in this genome.

4. **string Species**:
   - A random animal name given to the species this genome belongs to.
   - This name is entered at the moment the genome's species is determined.
   - This value is set by an external component.

5. **double Fitness**:
   - The score given to this genome by the Inspector component when its performance is evaluated.

6. **IAgent Agent**:
   - The agent is set by the factory in charge of creating genomes.
   - The agent is important because in the Agent the perceptors and actuators are defined and that is how the minimal genome genes are determined (input and output nodes).

#### Methods of the Genome

1. **IGenome CreateBasicGenome(IAgent agent)**:
   - Creates the basic genome based on the Agent's perceptors and actuators.
   - Initializes input nodes based on the agent's perceptors.
   - Initializes output nodes based on the agent's actuators.
   - Initializes a bias node.
   - No connections are created at this point; subsequent mutations will create connections.
   - **Returns**: The created basic genome.

2. **void Mutate(MutationLevels mutationLevel)**:
   - Mutates the current genome instance.
   - Applies mutations to genes based on the specified mutation level.
   - Mutations can include changes to gene properties, enabling/disabling genes, and adding/removing genes.
   - This method modifies the current instance and does not return any value.

3. **IGenome Crossover(IGenome partner, MutationLevels mutationLevel = 0)**:
   - Performs crossover with another genome to create a new genome.
   - Combines genes from both parent genomes based on the specified mutation level.
   - Ensures the resulting genome inherits traits from both parents.
   - **Returns**: The new genome created from the crossover.

4. **IGenome[] Speciate(int speciesCount, int siblingsCount = 0)**:
   - Creates a number of species from the current genome, ensuring that new genomes are of different species.
   - Divides the current genome into multiple species based on genetic diversity.
   - Optionally creates siblings within each species.
   - **Returns**: An array of new genomes representing different species.

5. **IGenome[] SpawnSiblingGenomes(int count, MutationLevels mutationLevels = MutationLevels.DEFAULT)**:
   - Creates a number of siblings from the current genome.
   - Siblings are mutated versions of the current genome but remain in the same species.
   - Applies mutations to create genetic diversity among siblings.
   - **Returns**: An array of sibling genomes.

6. **IGene.IConnectionGene AddConnection(int fromNode, int toNode, double weight)**:
   - Adds a new connection gene to the genome.
   - Creates a connection between the specified nodes with the given weight.
   - Ensures the connection adheres to the established genome policies.
   - **Returns**: The added connection gene.

7. **void AddNewConnection()**:
   - Adds a new connection gene to the genome.
   - Randomly selects nodes to create a new connection.
   - Ensures the connection adheres to the established genome policies.

8. **IGene.INodeGene AddNewNode()**:
   - Adds a new node gene to the genome.
   - Creates a new node and integrates it into the genome's structure.
   - Ensures the new node adheres to the established genome policies.
   - **Returns**: The added node gene.

9. **void FixStructure()**:
   - Fixes the structure of the genome to ensure it adheres to the established policies.
   - Resolves any structural issues, such as dangling genes or invalid connections.
   - Ensures the genome remains functional and consistent.

10. **double CalculateGeneticDifference(IGenome otherGenome)**:
    - Calculates the genetic difference between the current genome and another genome.
    - Evaluates genetic similarity and differences between the two genomes.
    - Returns a number between 0 and 1, where 0 means no difference and 1 means completely different.
    - **Returns**: A double indicating the genetic difference between the genomes.

11. **void AddRandomGene(MutationLevels mutationLevel)**:
    - Adds a random gene to the genome based on the mutation level.
    - Randomly selects a gene type and integrates it into the genome.
    - Ensures the new gene adheres to the established genome policies.

#### Using Partial Classes

To manage the complexity of the `Genome` class, we use partial classes to divide groups of functionalities by categories. This approach keeps related methods together while maintaining a clean and organized codebase.

### Example Structure with Partial Classes

1. **Genome.cs**: Core properties and methods.
2. **Genome.Mutation.cs**: Methods related to mutations.
3. **Genome.Structure.cs**: Methods related to structure management.


### Genome Policies

This document establishes the policies for dealing with genome structures in the TangoBotTrainer project. These policies serve as guidelines for creating and modifying genome structure elements.

#### Connection Policies

1. **About Cycles**:
   - A cycle is a training routine where a discrete set of skills are developed by the genome's neural network.
   - During a cycle, all genes created into the genome are marked with a ModuleId that indicates the cycle to which the gene belongs.
   - During the cycle, many iterations of training are run to develop, by genome evolution (evaluating, segregating, crossover and mutation), the skills or set of skills designated to be developed during the cycle.
   - After the cycle is completed, the genome's neural network should be proficient in executing the skills intended to be developed during such cycle.
   - While during a cycle, a genome's property called ModuleId is assigned a new value to mark each gene created within the cycle, with such ModuleId. When the cycle is done, all genes created during it will have been marked with the corresponding ModuleId. This will prevent that on future cycles, older modules that comprise genes well fit for their tasks, to be altered causing them to forget their skills. It also helps the genome structure methods to implement rules that prevent new modules from contaminating old well-established modules.

2. **About Modules**:
   - Modules consist of a group of genes developed during a specific cycle. During such cycle, the genome's neural network was being developed to perform a specific task or series of tasks.
   - Modules are used to separate the genome's neural network into different sections, each with its own set of genes.
   - Each gene has a property ModuleId which indicates the module to which the gene belongs.
   - New modules cannot influence older modules by creating connections from recent modules' nodes to older modules' nodes.
   - New modules can use information from older modules (only read) by creating connections FROM older modules' nodes TO recent modules' nodes. This ensures that skills learned in older modules cannot be overwritten by new modules and ensures that new modules can benefit from the learned skills of older modules.
   - Original nodes like input, output, and bias nodes are not part of any module. So any node of any module during any cycle can be connected from and to these nodes.

3. **Connection Rules**:
   - **Connection Direction**:
     - Connections within the same module must go from left to right, from a node in a layer with a lower layer number to a node on a layer with a higher layer number.
     - Connections can go from one node to another node on the same layer.
     - Input nodes are on layer 0, hidden nodes are on layers 1 to 99, and output nodes are on layer 100.
   - **Node Restrictions**:
     - The origin node cannot be an output node.
     - The origin node cannot be disabled.
     - Connections can have the same node as input and output.
   - **Existing Connections**:
     - Connections cannot be created if another connection already exists between the same nodes unless the previous connection gene is disabled.
     - A connection is considered as existing if the FromNode and ToNode properties are the same as the new connection.
     - Connections from and to the same node cannot be repeated unless the previous connection is disabled.
   - **Connection Weight**:
     - If the weight is -99, a random weight (double) between -1 and 1 is assigned at the time of creation.
   - **Connection State**:
     - Connections cannot be created in a disabled state. They can only be disabled by gene mutation.
   - **Connection Mutation**:
     - A connection can be mutated by changing its weight, enabling or disabling it, or changing the FromNode or ToNode properties.
     - The connection weight mutation consists of adding or subtracting a percentage value of the current weight. The percentage value is a random value between .0001 to a random value between 0.001 to a value that depends on the mutationLevel. A CLOSE_SIBLING mutationLevel will have a lower value than an INTERSPECIES mutationLevel, which might have a high value. This ensures that mutations are slight at low mutationLevels and more drastic at higher mutationLevels.

#### Node Policies

1. **Node Types**:
   - Special nodes (input, output, bias) cannot be added.
   - New nodes can be connected from and to nodes in the same layer.
   - New nodes can have one recursive connection to itself.

2. **Layer Assignment**:
   - The layer where the node is added is random from 1 to 99. Layer 0 is reserved for input nodes, and layer 100 is reserved for output nodes.

3. **Connections for New Nodes**:
   - New nodes should be connected from a node to the left that can be an input node or a hidden node.
   - New nodes can have connections from another node in another older module but cannot be connected to a node in another older module except output node.
   - New nodes can fall amid an existing connection. In this case, the existing connection changes the ToNode for the new node, and a new connection is created to connect the new node to the node formerly connected to the severed connection. The weight of the new connection is the same as the severed connection.

#### Removal Policies

1. **Removing Genes**:
   - Removing a gene of any type means setting the property enabled to false, effectively disabling the gene.
   - Disabled genes cannot be disabled again.
   - The removed genes keep their innovation number and are not removed from the genome.
   - Removed (disabled) genes can be later re-enabled by mutation.
   - When a gene is removed, the FixGenome method should be called to fix the genome structure.

2. **Removing Nodes**:
   - Only hidden nodes can be removed.
   - When a node is removed:
     - Connections that are connected to the node are also removed.
     - Connections that are connected from the node are also removed.
     - If the node is connected to itself, the connection is also removed.
     - If removing the node leaves other nodes orphaned (no incoming connections), new connections are created to connect the orphaned node to the node that the removed node was connected to, ensuring no duplicate connections.
     - If removing the node leaves other nodes leaf (no outgoing connections), new connections are created to connect the leaf node to the node to which the removed node was connected to, ensuring no duplicate connections.
   - After removing a node, the FixGenome method should be called to fix the genome structure.

3. **Removing Connections**:
   - Connections can only be removed if they are enabled.

#### Fixing Genome Structure

1. **Fixing Dangling Genes**:
   - The FixGenome method should be called to amend any dangling genes after any structural change in the genome.
   - For dangling connections (connections missing one or both nodes), the method should:
     - Reconnect the connection if it is missing the from node.
     - Reconnect the connection if it is missing the to node.
   - Remove nodes with no connections.
   - Reconnect orphaned or leaf nodes to ensure they have incoming and outgoing connections.

#### General Policies

1. **Random Selection**:
   - When selecting random nodes or connections, ensure the selection adheres to the specified restrictions (e.g., module, layer, side).

2. **Validation**:
   - Always validate the nodes and connections before creating or modifying them to ensure they adhere to the established policies.


### Mutation Policies

This document establishes the policies for mutating genome structures in the TangoBotTrainer project. These policies serve as guidelines for creating and modifying genome structure elements through mutation.

#### General Mutation Policies

1. **Mutation Levels**:
   - Mutations are categorized into different levels based on their impact: CLOSE_SIBLINGS, FAR_SIBLINGS, SPECIATION, EXTREME, and RANDOM.
   - CLOSE_SIBLINGS mutations are slight and have a lower impact, allowing the genome to mutate very gradually. This kind of mutation most likely will not render the mutated genome as another species.
   - FAR_SIBLINGS mutations introduce serious changes that might render the mutated genome as one of the same species but very different. It might end up being a different species but with similarities.
   - SPECIATION mutations have severe effects, rendering the mutated genome most likely into another species.
   - EXTREME mutations will do the most radical changes to the mutated genome, introduced to search for an out-of-the-box solution.
   - RANDOM mutations can be any of the previous mutation levels randomly.

2. **Random Selection**:
   - When selecting random nodes or connections for mutation, ensure the selection adheres to the specified restrictions (e.g., module, layer, side).

3. **Validation**:
   - Always validate the nodes and connections before and after mutation to ensure they adhere to the established policies.

#### Connection Mutation Policies

1. **Connection Weight Mutation**:
   - A connection's weight can be mutated by adding or subtracting a random N% of the current weight.
   - N% is established by selecting a random number between 1% to X%, where X% depends on the mutation level.
   - CLOSE_SIBLINGS mutations have a lower percentage value.
   - FAR_SIBLINGS mutations have a moderate percentage value.
   - SPECIATION mutations have a higher percentage value.
   - EXTREME mutations have the highest percentage value.

2. **Connection State Mutation**:
   - Connections can be enabled or disabled through mutation.
   - Connections cannot be created in a disabled state. They can only be disabled by gene mutation.

3. **Connection Node Mutation**:
   - A connection can be mutated by changing its FromNode or ToNode properties.
   - Ensure that the new nodes adhere to the connection rules specified in the Genome Policies.

#### Node Mutation Policies

1. **Node Addition**:
   - New nodes can be added through mutation.
   - The layer where the node is added is random from 1 to 99. Layer 0 is reserved for input nodes, and layer 100 is reserved for output nodes.
   - New nodes can be connected from and to nodes in the same layer.
   - New nodes can have one recursive connection to itself.

2. **Node Removal**:
   - Only hidden nodes can be removed through mutation.
   - When a node is removed:
     - Connections that are connected to the node are also removed.
     - Connections that are connected from the node are also removed.
     - If the node is connected to itself, the connection is also removed.
     - If removing the node leaves other nodes orphaned (no incoming connections), new connections are created to connect the orphaned node to the node that the removed node was connected to, ensuring no duplicate connections.
     - If removing the node leaves other nodes leaf (no outgoing connections), new connections are created to connect the leaf node to the node to which the removed node was connected to, ensuring no duplicate connections.
   - After removing a node, the FixGenome method should be called to fix the genome structure.

3. **Node Bias Mutation**:
   - A node's bias can be mutated by adding or subtracting a random N% of the current bias.
   - N% is established by selecting a random number between 1% to X%, where X% depends on the mutation level.
   - CLOSE_SIBLINGS mutations have a lower percentage value.
   - FAR_SIBLINGS mutations have a moderate percentage value.
   - SPECIATION mutations have a higher percentage value.
   - EXTREME mutations have the highest percentage value.

4. **Node Layer Mutation**:
   - Nodes can mutate by changing their layer number.
   - The node can be "moved" one layer or more to the right or to the left depending on the level of mutation.
   - This mutation should be rare and more rare to straddle over layers.
   - After this mutation, the structure must be fixed.

#### Gene Selection for Mutation

1. **Selection Percentage**:
   - The number of genes selected for mutation is determined by a percentage of the available genes.
   - The percentage is calculated by selecting a number from 1% to N%, where N% is determined from a random range influenced by the MutationLevel.

2. **MutationLevel Ranges**:
   - CLOSE_SIBLINGS: 1% to 5%
   - FAR_SIBLINGS: 1% to 10%
   - SPECIATION: 1% to 20%
   - EXTREME: 1% to 30%

3. **Selection Process**:
   - Determine the range for N% based on the MutationLevel.
   - Select a random percentage within the range.
   - Shuffle the Genes collection, excluding genes from other modules.
   - Calculate the number of genes to be selected based on the percentage.
   - Randomly select the calculated number of genes from the top of the shuffled list for mutation.
   - Mutate each selected gene.

#### Mutation Coefficients

1. **ValueMutation Coefficients**:
   - CLOSE_SIBLINGS: 0.1 to 0.2
   - FAR_SIBLINGS: 0.2 to 0.5
   - SPECIATION: 0.5 to 1.0
   - EXTREME: 1.0 to 2.0

2. **StructuralMutation Coefficients**:
   - CLOSE_SIBLINGS: 0.05 to 0.1
   - FAR_SIBLINGS: 0.1 to 0.3
   - SPECIATION: 0.3 to 0.6
   - EXTREME: 0.6 to 1.0

#### Selecting Elements for Mutation

1. **Selection Percentage**:
   - A N% of the items of the shuffled eligible genes collection is selected for mutation.
   - The N% is determined by randomly selecting a value from 1 to X, where X is determined by the structural mutation level.

#### Mutating Gene Value (bias, weight)

1. **Value Mutation**:
   - A N% of the original value is added (or subtracted).
   - The N% is determined by randomly selecting a value from 1 to X, where X is determined by the value mutation level.

#### Fixing Genome Structure

1. **Fixing Dangling Genes**:
   - The FixGenome method should be called to amend any dangling genes after any structural change in the genome.
   - For dangling connections (connections missing one or both nodes), the method should:
     - Reconnect the connection if it is missing the from node.
     - Reconnect the connection if it is missing the to node.
   - Remove nodes with no connections.
   - Reconnect orphaned or leaf nodes to ensure they have incoming and outgoing connections.


### 2.4 Neural Network Runtime (NNRT)
Describe the Neural Network Runtime component, its role, and how it brings the static NN metadata to life.

## 3. Design Guidelines

### 3.1 Agent Design
Provide design guidelines for the Agent, including its properties, methods, and interactions.

### 3.2 Neural Network Design
Outline the design principles for the Neural Network, including node and connection structures.

### 3.3 Genome Design
Detail the design guidelines for the Genome, including gene properties and mutation mechanisms.

### 3.4 Neural Network Runtime Design
Describe the design principles for the Neural Network Runtime, including input processing and output generation.

## 4. Implementation Details

### 4.1 Agent Implementation
Provide implementation details for the Agent, including code examples and best practices.

### 4.2 Neural Network Implementation
Detail the implementation of the Neural Network, including node and connection handling.

### 4.3 Genome Implementation
Describe the implementation of the Genome, including gene creation and mutation processes.

### 4.4 Neural Network Runtime Implementation
Provide implementation details for the Neural Network Runtime, including input processing and output handling.

## 5. Abstract Concepts

### 5.1 Evolutionary Algorithms
Explain the concept of evolutionary algorithms and their application in the project.

### 5.2 Neural Network Training
Describe the process of training neural networks and its significance in the project.

### 5.3 Perceptors and Actuators
Define the roles of Perceptors and Actuators in the project and their interactions with the Agent.

### 5.4 Environment Interaction
Explain how the Agent interacts with the environment and the importance of this interaction.

### 5.5 Evaluators
Learning is guided by Evaluators that are introduced in the process at cycles of development or evolution. At each cycle, one or more Evaluators are introduced. The Evaluators curate certain behaviors and discourage others by providing rewards or punishments to the running NN as fitness scores. Such fitness scores will eventually be held by the neural network's genome. Learning concentrates on what is encouraged or discouraged by means of the Evaluators.
Evaluators are not intended to be modified between cycles but between Epochs. Generally, it is not known to what extent an evaluator is responsible for the performance of the neural network until the Epoch ends. The adjustment of an evaluator that works on cycle 5 of a 10-cycle Epoch can be adjusted. This means that the training during other cycles besides cycle 5 will run very fast because the NN is already fit, but during cycle 5, it will have to be developed. Therefore, there is no difference in modifying the Evaluator. In conclusion, it is preferable to study the performance reports and how each evaluator directed the development at the end of the Epoch.
Evaluators direct the development of the Neural Network by rewarding or penalizing observed behavior.
They assess the performance by executing a logic that, in general, compares some information produced by the NN's work against its metrics for that specific case. For instance, an Evaluator that focuses on account drawdown might have a metric that tolerates a max drawdown of -8%. If at any time during the Iteration the drawdown falls to -8% or below, the Evaluator stores a score that will be injected into the current NN's genome. Later, when selection comes, the affected genome is compared to its siblings by the fitness score. This is done when the iteration ends because it makes no sense to qualify the performance amid the Iteration since the evolution selection, crossover, and mutation happen at the last phases of the Iteration.
Evaluators can be set to assess the performance at the end of each Cursor process. This responds to the need of providing feedback to the NN to develop a predictive behavior. This type of Evaluator assesses decisions taken amid the Iteration and provides the NN with feedback. This kind of Evaluator can teach the NN to determine how likely it is that something happens before it happens by observing patterns that normally precede such occurrences. For example, an Evaluator can teach the NN to predict a real breakout from a resistance level by notifying the NN of the occurrence. This will prompt a mechanism to associate the feedback with the patterns registered in previous input data context by means of an Anticipatory Memory System. This system maintains a context of what happens periods before the feedback occurs. A memory management mechanism creates an entry in a memory repository that is associated with the feedback. The next time the same pattern becomes present, the feedback will be triggered but this time as a special input node that eventually other parts of the NN will feed from to make decisions that deal with the occurrence.
Other types of Evaluators are designed to develop skills that do not end up in output nodes because these are intended to produce a value that should be used by other NN structures as input. The Evaluator defines the Module Output Node (MON). Other NN structures can connect to this MON.
An Evaluator is a metadata that defines: Name, Description for reporting purposes, type (Iteration, feedback, partial module), Metrics, and a logic (JavaScript, Python, DLL, etc.) that is injected with a wide context of the system, the data, the environment, etc., and is responsible for performing the comparisons and publishing scores to the TestingBooth.
### 5.6 Evolution Process
The evolution process is divided into periods or timeframes at different levels.

#### 5.6.1 Epoch
An Epoch is a period where the final product is a fully functional NN. Generally, a single Epoch should be enough to produce a production-ready NN. However, if previously developed skills need to be improved or change their behavior, a new Epoch can be started with revised versions of the evaluators or even adding new input nodes to the Agent. In any case, the Epoch starts and cycles are run.

#### 5.6.2 Cycle
There can be one or many cycles within an Epoch. A cycle's purpose is for the NN to develop one or more discrete skills, enabling the trainer to gradually develop simple skills that over time will manifest into more complex skills and behaviors. Associated with each cycle is one or more Evaluators designed to observe the performance of the NN regarding a specific skill or set of skills. Once the established metrics in the evaluator are reached, the cycle is over, and control passes to the Epoch again. The cycle comprises steps where a genome created at the project initialization or inherited from previous cycles is submitted for iterations, which are periods executed as many times as needed within a Cycle period until the NN reaches the expected performance.

#### 5.6.3 Iteration
Each iteration comprises the following steps: Evaluation, Selection, Crossover, Mutation, and deployment for evaluation. Evaluation is done by the Evaluators. Selection consists of selecting the best performers from each species, crossover is the reproduction of the best performers' couples. The children are mutated and then deployed for testing. This iteration is repeated as needed until the goal of the Evaluators is reached. If reached, then the cycle ends. Another cycle starts, or if it is the final cycle, then control is passed to Epoch.

#### 5.6.4 Cursor
Within the Iteration, another recursive set of steps is executed, called Cursor. Cursor is repeated as long as there is testing data in the data set. When there is no more data or the end of data is reached, then control is passed to Iteration. Cursor happens during the Evaluation phase of Iteration. At each Cursor phase, data is passed to the agent, normalized, input to the NNRT, and the agent captures the output through its corresponding Actuators, turning it into action. Some evaluators might evaluate performance at the cursor level to provide feedback. Once the Cursor phase ends, a new data point is prepared and sent to the agent perceptors, and so on. Cursor processes one data point at a time in the case that it is databased. If the data is contained in a query table, then each record is a data point. The training booth picks one data point at a time, sends it to the agent as a single package. The agent is responsible for separating each field and directing it to the matching perceptor.

### 5.7 Anticipatory Memory System (AMS)

The Anticipatory Memory System (AMS) is a contextual, evolutionary memory system that enables TangoBot to anticipate extraordinary market events before they occur. Unlike classical neural network memory mechanisms (e.g., LSTMs, RNNs), AMS functions by storing pre-event conditions and recalling them when similar patterns emerge, influencing decision-making through predictive feedback.
AMS is genetically controlled, meaning its structure evolves over generations. If AMS proves beneficial to TangoBot’s trading success, it persists and refines itself. If it does not contribute meaningfully, it mutates out of the genome, preventing unnecessary complexity.

#### 5.7.1 Core Principles

| Feature                | Description                                                                                                               |
|------------------------|---------------------------------------------------------------------------------------------------------------------------|
| Contextual Learning    | AMS stores market conditions preceding major events (e.g., price crashes, volatility spikes).                             |
| Predictive Feedback    | When similar conditions reappear, AMS generates anticipatory warnings with a strength proportional to pattern similarity. |
| Self-Generated Input   | AMS feeds its warnings into a Feedback Node (FN), which acts as the neural network’s "pain/pleasure receptor."            |
| Evolutionary Refinement| AMS adapts over generations, optimizing what it stores and when it triggers warnings.                                     |
| Genetic Control        | AMS’s memory slot capacity is determined by genetic mutations, preventing unnecessary memory depth.                       |
| Taxation for Efficiency| AMS is heavily taxed per slot and per recall event to ensure it is only retained when truly beneficial.                   |


#### 5.7.2 AMS Structure and Components
A. Memory Nodes (MN) - Storing Context and Feedback
Memory Nodes are not standard NN nodes; they operate as context-capturing elements that associate market conditions with positive or negative feedback.
Each Memory Node consists of:
•	Stored Context Elements – Historical data points leading up to an event.
•	Feedback Association – A memory’s correlation to past profits/losses.
•	Context Matching Algorithm – Determines how closely a new market condition matches a past one.
•	Feedback Strength Calculation – Generates anticipatory warnings scaled by similarity to past events.
Context Storage Example
AMS records the last N timeframes before a large price drop (-3%).

| Timeframe | Input Node            | Value    |
|-----------|-----------------------|----------|
| T-10      | Resistance Level      | 4200     |
| T-8       | RSI                   | 80       |
| T-6       | Volatility            | 25%      |
| T-4       | Downward Volume Spike | 1.5x avg |
| T-2       | VIX                   | 30       |


This context is stored with an initial correlation of 1.0 to the loss event. Over time, AMS refines correlation values to retain only the most predictive elements.
B. Feedback Nodes (FN) - Pain/Pleasure Receptors
Since TangoBot lacks biological pain receptors, Feedback Nodes (FN) serve as dedicated inputs that receive both:
1.	Direct feedback from Evaluators (realized profit/loss).
2.	Predicted feedback from AMS (anticipated market conditions).
Each Feedback Node:
•	Injects memory-driven anticipation as a self-generated input.
•	Distributes feedback to influence trading decisions.
•	Competes for survival through evolutionary selection.
Example FN Output in a Market Downturn

| Timeframe | Matching Context %  | AMS Feedback Output    |
|-----------|---------------------|------------------------|
| T-10      | 35% match           | Weak warning (0.2)     |
| T-5       | 65% match           | Moderate warning (0.5) |
| T-2       | 90% match           | Strong warning (0.9)   |
| T-1       | 98% match           | Critical warning (1.0) |


This feedback is used by the NN to adjust position sizing, hedge risk, or avoid trades entirely.

C. Context Matching and Feedback Scaling AMS uses pattern recognition to determine if an incoming market condition resembles a past extreme event. It employs:
•	Vector Similarity (Cosine, Euclidean Distance)
•	Feature Weighting (Some signals matter more than others)
•	Adaptive Learning (Correlations adjust over time)

Full vs. Partial Matching:
•	Full Match → Memory is fully relevant, feedback is strong.
•	Partial Match → Feedback is scaled proportionally.
•	No Match → No feedback is produced.
Example: AMS detects a 75% match to a past loss event → Generates 0.75 intensity warning.

feedback_strength = memory_node.calculate_feedback_strength(current_context)
feedback_node.receive_feedback(feedback_strength)

#### 5.7.3 AMS Evolutionary Process
AMS does not randomly evolve like traditional NN structures. Instead, it competes for survival in an evolutionary framework.
Genetic Mutation of AMS
•	Memory Slot Count is Genetically Determined → More slots allow deeper memory but increase computational cost.
•	Slot Count Mutations → AMS may gain or lose slots based on utility.
•	Structural Evolution → If AMS proves beneficial, its structure refines across generations.
Selection Criteria for AMS Survival

| Condition                                      | Action                          |
|------------------------------------------------|---------------------------------|
| AMS reduces losses or increases gains          | Retain in genome                |
| AMS rarely activates                           | Reduce slot count               |
| AMS activates too late to be useful            | Improve recall weighting        |
| AMS increases caution to an unnecessary degree | Adjust correlation parameters   |
| AMS proves ineffective over generations        | Mutation removes AMS            |


#### 5.7.4 AMS Taxation for Efficiency
Since AMS is computationally expensive, it incurs a heavy tax, discouraging wasteful memory retention.
Taxation Model
[ T(AMS) = C_1 + C_2 \times \text{Slot Count} + C_3 \times \text{Recall Frequency} + C_4 \times \text{Complexity} ]
Where:
•	( C_1 ) = Base AMS tax.
•	( C_2 ) = Cost per memory slot.
•	( C_3 ) = Cost per feedback recall.
•	( C_4 ) = Complexity penalty for deep memory systems.
Example Tax Calculation
•	AMS with 20 slots, recalls 5 times per day, and tracks 10 context elements:

 T(AMS) = 10 + 0.5 \times 20 + 0.1 \times 5 + 0.2 \times 10 = 10 + 10 + 0.5 + 2 = 22.5

•	If AMS fails to improve trading, this cost forces its removal.

#### 5.7.5 AMS Summary Table
| Feature | Description | |---------|------------| | Predictive Capability | Warns the NN of extreme market events before they happen. | | Contextual Learning | Stores market conditions before feedback events to recognize future patterns. | | Self-Generated Inputs | Feeds predictive feedback to the NN via Feedback Nodes (FN). | | Genetic Mutation Control | AMS evolves or disappears based on its usefulness. | | Memory Slot Taxation | Slots are taxed per unit to prevent unnecessary depth. | | Selection Pressure | AMS is removed if it does not improve TangoBot’s performance. |

#### 5.7.6 Potential Benefits of Implementing AMS
1.	Predictive Capability:
•	AMS can warn the neural network (NN) of extreme market events before they happen by recognizing patterns that precede such events. This allows TangoBot to take preemptive actions to mitigate risks or capitalize on opportunities.
2.	Contextual Learning:
•	AMS stores market conditions preceding major events (e.g., price crashes, volatility spikes) and uses this historical context to improve decision-making. This helps TangoBot to better understand and react to market dynamics.
3.	Enhanced Decision-Making:
•	By feeding predictive feedback into the NN via Feedback Nodes (FN), AMS helps the NN to make more informed decisions. This can lead to improved trading performance and reduced losses.
4.	Evolutionary Refinement:
•	AMS adapts over generations, optimizing what it stores and when it triggers warnings. This continuous improvement ensures that AMS remains relevant and effective in changing market conditions.
5.	Genetic Control:
•	AMS’s memory slot capacity is determined by genetic mutations, preventing unnecessary memory depth. This ensures that AMS remains efficient and does not consume excessive computational resources.
6.	Taxation for Efficiency:
•	AMS is heavily taxed per slot and per recall event to ensure it is only retained when truly beneficial. This discourages wasteful memory retention and promotes the use of only the most valuable memory slots.
7.	Improved Risk Management:
•	AMS can help TangoBot to better manage risks by providing early warnings of potential market downturns or other adverse events. This allows TangoBot to adjust its trading strategies accordingly.
8.	Competitive Advantage:
•	Implementing AMS can give TangoBot a competitive edge by enabling it to anticipate and react to market events more effectively than other trading systems that do not have such predictive capabilities.
Overall, AMS enhances TangoBot's ability to learn from past market conditions, predict future events, and make more informed trading decisions, leading to improved performance and reduced risks.

## 6. Development Practices

### 6.1 Coding Standards
Outline the coding standards and best practices to be followed by the development team.

### 6.2 Testing and Validation
Describe the testing and validation processes to ensure the quality and reliability of the project.

### 6.3 Version Control
Provide guidelines for version control and collaboration using tools like Git.

### 6.4 Documentation
Emphasize the importance of documentation and provide guidelines for maintaining comprehensive project documentation.

## 7. TangoBot Training Project (TBTP)

### 7.1 Project Structure

A TangoBot Training Project (TBTP) is a set of artifacts that encompasses the necessary objects, metadata, scripts, DLL libraries, configurations, etc., needed to produce a neural network (NN) capable of production deployment.

#### 7.1.1 Static Elements

1. **Agent Definition Metadata**:
   - Metadata files that define the agent's properties, perceptors, and actuators.
   - These files are used to initialize the agent and its interactions with the environment.

2. **Scripts and/or DLL Libraries**:
   - Scripts and DLL libraries that implement the evaluators.
   - These evaluators are responsible for assessing the performance of the neural network during training.

3. **Training Environment**:
   - The environment in which the training takes place.
   - For example, in a trading training project, this would be the Trading Platform.

4. **Data Provider Definition Metadata**:
   - Metadata files that define the data providers.
   - These files specify the sources of data used for training the neural network.

#### 7.1.2 Non-Static Elements

These elements will appear during and after the training process.

1. **Genome Definitions**:
   - Definitions of the genomes used in the training process.
   - These files contain the genetic information of the neural networks being evolved.

2. **Neural Network Definitions**:
   - Definitions of the neural networks produced during training.
   - These files contain the architecture and weights of the trained neural networks.

3. **Logs**:
   - Log files generated during the training process.
   - These logs provide detailed information about the training progress, performance metrics, and any issues encountered.

4. **Other Artifacts**:
   - Additional files and artifacts generated during the training process.
   - These may include intermediate results, checkpoints, and other relevant data.

### 7.2 Directory Structure

A typical TBTP directory structure may look like this:

/TBTP
|-- /AgentDefinition
|-- /Scripts
|-- /Libraries
|-- /TrainingEnvironment
|-- /DataProviders
|-- /Genomes
|-- /NeuralNetworks
|-- /Logs
|-- /Artifacts

### 7.3 Key Components

1. **Agent**:
   - The agent is the entity being trained. It interacts with the environment through its perceptors and actuators.
   - The agent definition metadata specifies the agent's properties and configuration.

2. **Evaluator**:
   - The evaluator assesses the performance of the neural network during training.
   - It can be implemented as scripts or DLL libraries.

3. **Training Environment**:
   - The environment in which the agent operates and learns.
   - It provides the context and data necessary for training the neural network.

4. **Data Provider**:
   - The source of data used for training the neural network.
   - The data provider definition metadata specifies the data sources and their configuration.

5. **Genomes**:
   - The genetic representations of the neural networks being evolved.
   - These definitions are used to create and modify neural networks during training.

6. **Neural Networks**:
   - The trained neural networks produced during the training process.
   - These definitions contain the architecture and weights of the neural networks.

7. **Logs**:
   - Detailed records of the training process, including performance metrics and any issues encountered.

8. **Other Artifacts**:
   - Additional files and artifacts generated during the training process, such as intermediate results and checkpoints.

## 8. Application Components

### 8.1 UI
The UI is a Visual Studio-like interface that presents the project as a tree structure and provides a work area to configure each artifact in the project. For instance, among the listed artifacts, there is an Agent artifact. When focused, it presents a form with necessary fields to edit its configuration. A property pane might also display contextual information about the artifact. These can be static or dynamic as defined in the TBTP section.

### 8.2 Workflow
This component orchestrates the training actions from the beginning based on the TBTP artifacts.

### 8.3 TestingBooth
Wraps an Agent being evaluated, providing access to data and injecting context information, fitness, etc., to the Agent being tested. The TestingBooth is responsible for serving one data point at a time regardless of the source of the data. This is done by accessing a component named DataManager that is in charge of sourcing the data, performing any data operations like merging, aggregation, segregation, etc., and making it available for the TestingBooth. Downstream from the TestingBooth, the process should remain standard for the agent, which receives a package of data containing the data point. The agent has the responsibility to digest the data and perform pertinent granulations.

### 8.4 Agent
Wraps the Neural Network runtime. Normalizes data for input, interprets output nodes activation to turn them into actions.

### 8.5 Environment
Is what the agent interacts with. The agent gets information from the outside world (the training booth) and executes actions on the environment. In this case, the environment is a Trading Platform Interface that accepts specific actions and can expose data (like account balance) that the agent can pick.

### 8.6 Neural Network Runtime (NNRT)
The Neural Network Runtime (NNRT) component brings the static Neural Network (NN) metadata to life. The NN metadata, which is built after the corresponding genome, defines nodes and connections. The NN is effectively the phenotype, or the result of the genotype, which is the genome.

#### 8.6.1 Properties of the NNRT

1. **Dictionary<string, double> Input**:
   - A collection of input values received from the agent's perceptors.
   - Each input node maintains a **buffer** of past values for historical context.
   - The buffer size is **genetically determined** and varies per node.
   - When new data arrives, the oldest value is purged, maintaining a rolling set of entries.

2. **Dictionary<string, double> Output**:
   - A collection of output values processed by the output nodes.
   - The result of the activation function is passed to all outgoing connections until the output nodes are reached.
   - The change in the output property is notified so that the agent can listen to the property change and pass the values contained in the output to the respective Actuator.
   - Outputs from earlier modules are **retained in Pre-Activation Buffers** until all dependent computations are complete.

3. **List<Node> Nodes**:
   - A collection of nodes in the neural network.
   - Each node has a unique identifier, a bias, and a list of outgoing connections.
   - Each node has a **genetic property determining how it consumes buffered input data**:
     - **Scalar Mode**: Uses only the latest value.
     - **Vector Mode**: Processes the entire buffer as an input array.
   - The consumption mode is **determined by genetic mutation and evolves over time**.

4. **List<Connection> Connections**:
   - A collection of connections between nodes in the neural network.
   - Each connection has a source node, a destination node, and a weight.

5. **Modules**:
   - The network is developed in **modules**, each assigned a unique **ModuleId**.
   - Modules are trained **sequentially** and cannot be altered by later modules.
   - **Nodes from newer modules cannot modify nodes in older modules**.
   - Some modules are **truncated**, meaning they do not reach an output node but serve as intermediary computation layers.
   - Each module is developed within a cycle that defines **evaluation metrics, output criteria, and learning objectives**.

### 8.7 Evaluators
Evaluate the behavior of the neural network or parts (modules) of the neural network and provide fitness scores to direct the evolution of the NN.

### 8.8 DataManager
Is in charge of providing the data against which the NN will be tested.

## 9. Appendices

### 9.1 Glossary of Terms
Provide a glossary of terms used in the project to ensure a common understanding among team members.

### 9.2 References and Resources
List references and resources for further reading and research.

### 9.3 Example Code Snippets
Include example code snippets to illustrate key concepts and implementation details.
